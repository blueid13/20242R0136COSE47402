{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 매 시도마다 돌릴 것들"
      ],
      "metadata": {
        "id": "g7qZZJCL_NvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install datasets --quiet\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# 데이터셋 경로 설정\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/data/'  # 이미지 및 텍스트 파일 폴더\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import  AutoConfig\n",
        "from scipy.special import softmax\n",
        "import shutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMkpM8rP_RdE",
        "outputId": "3daf1647-1d80-4582-aa12-48cd7e196d36"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "qz8G_fuW_v1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 데이터 로드\n",
        "texts_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/texts.csv')\n",
        "label_txt = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/label.txt', sep='\\t', header=0)\n",
        "label_txt.columns = ['ID', 'Annotator1', 'Annotator2', 'Annotator3']\n",
        "label_jpg = label_txt.copy()\n",
        "\n",
        "def pre_text(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "texts_df['Text'] = texts_df['Text'].apply(pre_text)"
      ],
      "metadata": {
        "id": "A_k8XYMJifKc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {'positive': 1, 'neutral':0, 'negative': -1}\n",
        "def pre_labeltext(text):\n",
        "    new_text = text.split(\",\")[0]\n",
        "    return dic[new_text]\n",
        "label_txt['Annotator1'] = label_txt['Annotator1'].apply(pre_labeltext)\n",
        "label_txt['Annotator2'] = label_txt['Annotator2'].apply(pre_labeltext)\n",
        "label_txt['Annotator3'] = label_txt['Annotator3'].apply(pre_labeltext)\n",
        "label_txt['label'] = label_txt['Annotator1'] + label_txt['Annotator2'] + label_txt['Annotator3']\n",
        "label_txt['label'] = label_txt['label']/3\n",
        "label_txt = label_txt.drop(['Annotator1', 'Annotator2', 'Annotator3'], axis=1)"
      ],
      "metadata": {
        "id": "JZqXgtrVAiYc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_labeltext(text):\n",
        "    new_text = text.split(\",\")[1]\n",
        "    return dic[new_text]\n",
        "label_jpg['Annotator1'] = label_jpg['Annotator1'].apply(pre_labeltext)\n",
        "label_jpg['Annotator2'] = label_jpg['Annotator2'].apply(pre_labeltext)\n",
        "label_jpg['Annotator3'] = label_jpg['Annotator3'].apply(pre_labeltext)\n",
        "label_jpg['label'] = label_jpg['Annotator1'] + label_jpg['Annotator2'] + label_jpg['Annotator3']\n",
        "label_jpg['label'] = label_jpg['label']/3\n",
        "label_jpg = label_jpg.drop(['Annotator1', 'Annotator2', 'Annotator3'], axis=1)"
      ],
      "metadata": {
        "id": "IqwSsrVkApKb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨 데이터와 텍스트 데이터 병합\n",
        "merged_text = pd.merge(label_txt, texts_df, on='ID')"
      ],
      "metadata": {
        "id": "DretnLVMj9K-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 라벨링-모델 일치율 확인(만족스럽다)"
      ],
      "metadata": {
        "id": "rdeeeqfcT90t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = merged_text['Text'].tolist()\n",
        "actual_labels = merged_text['label'].tolist()\n",
        "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHRKiZFWH5cA",
        "outputId": "49429aa0-ffbe-4dfb-e807-15c97e6b4ff0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 사이즈 설정\n",
        "batch_size = 16\n",
        "sentiment_scores = []\n",
        "\n",
        "for i in tqdm(range(0, len(texts), batch_size)):\n",
        "    batch_texts = texts[i:i+batch_size]\n",
        "    encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    encoded_input.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded_input)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    scores = probabilities.cpu().numpy()\n",
        "\n",
        "    # 감성 점수 계산: (긍정 확률 - 부정 확률)\n",
        "    pos_probs = scores[:, 2]  # 긍정 확률\n",
        "    neg_probs = scores[:, 0]  # 부정 확률\n",
        "    sentiments = pos_probs - neg_probs  # 감성 점수\n",
        "    sentiment_scores.extend(sentiments)\n",
        "\n",
        "# 실제 라벨과 예측된 감성 점수 변환\n",
        "actual_labels = np.array(actual_labels)\n",
        "sentiment_scores = np.array(sentiment_scores)\n",
        "\n",
        "# L2 Loss 계산\n",
        "l2_loss = np.mean((sentiment_scores - actual_labels) ** 2)\n",
        "print(f\"L2 Loss: {l2_loss}\")\n",
        "\n",
        "# 상관계수 계산\n",
        "correlation = np.corrcoef(actual_labels, sentiment_scores)[0, 1]\n",
        "print(f\"상관계수: {correlation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxDmLDnti0l-",
        "outputId": "901f0518-cfc9-4164-e4bd-c5f89b72af5e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1225 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "100%|██████████| 1225/1225 [00:18<00:00, 66.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Loss: 0.22832325209626356\n",
            "상관계수: 0.5043935582499393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range (19600):\n",
        "    if sentiment_scores[i] * actual_labels[i] < 0:\n",
        "        count+=1\n",
        "count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoaKrFPvKvUI",
        "outputId": "45e6be2f-a35f-4ad1-b8af-739a2884a26d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1704"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_scores[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIsGSj3MK96p",
        "outputId": "1c2ef60d-a37b-464a-948a-7aa92f48f8ee"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15349671"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 설명"
      ],
      "metadata": {
        "id": "rMZJzuuVUFOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#자기 전에 켜 둘 것\n",
        "source_dir = '/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/data'\n",
        "\n",
        "# 대상 데이터 경로 (Colab 로컬 스토리지)\n",
        "target_dir = '/content/data'\n",
        "\n",
        "# 대상 폴더가 없으면 생성\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "loader = label_txt['ID'].tolist()\n",
        "\n",
        "for filename in tqdm(loader):\n",
        "    filename_with_ext = f\"{filename}.jpg\"\n",
        "    src_file = os.path.join(source_dir, filename_with_ext)\n",
        "    dst_file = os.path.join(target_dir, filename_with_ext)\n",
        "\n",
        "    try:\n",
        "        shutil.copy2(src_file, dst_file)\n",
        "    except Exception as e:\n",
        "        print(f\"파일 복사 중 오류 발생: {filename_with_ext}, 오류: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrWxnT8MfbA7",
        "outputId": "959eaddf-ccb9-4234-a1a7-3cec7ac4fbd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 139/19600 [00:46<1:42:06,  3.18it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = np.array(sentiment_scores)\n",
        "sim = label_jpg['label'] * label_txt['label'] > 0\n",
        "label_jpg['pred'] = scores\n",
        "input_df = label_jpg[sim]"
      ],
      "metadata": {
        "id": "Z5VMAsSNXP2h"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GcDaSHbVhSR5"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader[0]"
      ],
      "metadata": {
        "id": "gxrlz6_lhcVF",
        "outputId": "4e293935-d883-43e1-b783-0054b617a195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2499"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L_aYgG8WhdxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}