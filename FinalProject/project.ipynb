{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install datasets --quiet\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# 데이터셋 경로 설정\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/data/'  # 이미지 및 텍스트 파일 폴더\n",
        "\n",
        "# 텍스트 전처리 함수\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # 소문자로 변환\n",
        "    text = ''.join(e for e in text if e.isalnum() or e.isspace())  # 특수문자 제거\n",
        "    return text\n",
        "\n",
        "# 커스텀 데이터셋 클래스 정의\n",
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        self.folder_path = folder_path\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # 폴더 내 파일 목록 가져오기\n",
        "        file_names = [f.split('.')[0] for f in os.listdir(folder_path) if f.endswith('.jpg')]  # .jpg 파일 기준\n",
        "        unique_ids = set(file_names)  # 중복 제거\n",
        "\n",
        "        for file_id in unique_ids:\n",
        "            # 이미지와 텍스트 파일 경로 생성\n",
        "            image_file = os.path.join(folder_path, f\"{file_id}.jpg\")\n",
        "            text_file = os.path.join(folder_path, f\"{file_id}.txt\")\n",
        "\n",
        "            # 이미지와 텍스트 파일이 모두 있는 경우 로드\n",
        "            if os.path.exists(image_file) and os.path.exists(text_file):\n",
        "                self.data.append((image_file, text_file))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_file, text_file = self.data[idx]\n",
        "\n",
        "        # 이미지 로드 및 전처리\n",
        "        image = cv2.imread(image_file)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"이미지를 로드할 수 없습니다: {image_file}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # RGB로 변환\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # 기본 변환 적용\n",
        "            image = cv2.resize(image, (224, 224))\n",
        "            image = image / 255.0  # 정규화\n",
        "            image = torch.tensor(image, dtype=torch.float32)\n",
        "            image = image.permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
        "\n",
        "        # 텍스트 로드 및 전처리\n",
        "        with open(text_file, 'r') as file:\n",
        "            raw_text = file.read().strip()\n",
        "        text = preprocess_text(raw_text)\n",
        "\n",
        "        return image, text\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = ImageTextDataset(data_path, transform=transform)"
      ],
      "metadata": {
        "id": "KmFBTZ5dJqdV",
        "outputId": "b85bb277-83b7-4a2c-8ccd-da971c8169f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "ZCQHOIcmJucu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/dataset'\n",
        "dataset.save_to_disk(save_path)"
      ],
      "metadata": {
        "id": "qEIMGEi3SjAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "# 저장된 데이터셋 로드\n",
        "loaded_dataset = load_from_disk(save_path)\n",
        "print(loaded_dataset)"
      ],
      "metadata": {
        "id": "qxDuwjnESDKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. 데이터셋 전처리 (토크나이저 사용)\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# 텍스트 토크나이즈\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# PyTorch DataLoader 생성\n",
        "def collate_fn(batch):\n",
        "    # 텍스트 입력과 이미지를 함께 반환\n",
        "    input_ids = torch.stack([b[\"input_ids\"][0] for b in batch]).to(device)\n",
        "    attention_mask = torch.stack([b[\"attention_mask\"][0] for b in batch]).to(device)\n",
        "    images = torch.tensor([b[\"image\"] for b in batch]).to(device)\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, images\n",
        "\n",
        "# DataLoader 설정\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(tokenized_dataset, batch_size=batch_size, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "7CvAzXLbRXcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. 모델 예측 (배치 처리)\n",
        "all_labels = []\n",
        "all_probabilities = []\n",
        "\n",
        "for batch in data_loader:\n",
        "    inputs, _ = batch  # 텍스트 입력만 사용\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # 소프트맥스 확률 계산\n",
        "    probabilities = F.softmax(outputs.logits, dim=1)\n",
        "    labels = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
        "\n",
        "    # 결과 저장\n",
        "    all_labels.extend(labels)\n",
        "    all_probabilities.extend(probabilities.cpu().numpy())"
      ],
      "metadata": {
        "id": "xQmB2iMCRZet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 결과 매핑 및 출력\n",
        "label_map = {0: \"negative\", 1: \"positive\"}\n",
        "all_labels = [label_map[label] for label in all_labels]\n",
        "\n",
        "# 첫 번째 배치 결과 출력\n",
        "for text, label, prob in zip(texts[:batch_size], all_labels[:batch_size], all_probabilities[:batch_size]):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Sentiment: {label}\")\n",
        "    print(f\"Probabilities: {prob}\")"
      ],
      "metadata": {
        "id": "UZEnXvlmRaof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 여기까지 1차 task: 전처리+추후 학습에 쓸 이미지 pos/neg 비율(이건 러프하게 잡아도 될 것 같음. 어차피 학습을 서로 돌리면서 맞출 거니까)"
      ],
      "metadata": {
        "id": "7qAC4454Ka4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ALBert 모델과 토크나이저 로드\n",
        "albert_model_name = \"albert-base-v2\"  # Hugging Face 모델 이름\n",
        "tokenizer = AutoTokenizer.from_pretrained(albert_model_name)\n",
        "albert_model = TFAutoModel.from_pretrained(albert_model_name)\n",
        "\n",
        "# 입력 텍스트 처리 함수\n",
        "def preprocess_texts(texts, max_length=128):\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "    # 'token_type_ids' 제거\n",
        "    return {\n",
        "        \"input_ids\": tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"]\n",
        "    }\n",
        "\n",
        "# BiLSTM 모델 정의\n",
        "def build_bilstm_model(albert_model, lstm_units=128):\n",
        "    # ALBert 입력\n",
        "    input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
        "    attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "    # ALBert 임베딩 추출\n",
        "    albert_outputs = albert_model(input_ids, attention_mask=attention_mask)\n",
        "    albert_embeddings = albert_outputs.last_hidden_state  # (batch_size, seq_length, hidden_size)\n",
        "\n",
        "    # BiLSTM 적용\n",
        "    bilstm = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(lstm_units, return_sequences=False)\n",
        "    )(albert_embeddings)\n",
        "\n",
        "    # 최종 출력\n",
        "    output = tf.keras.layers.Dense(lstm_units, activation=\"relu\")(bilstm)\n",
        "\n",
        "    # 모델 정의\n",
        "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "    return model\n",
        "\n",
        "# 텍스트 예제\n",
        "texts = [\n",
        "    \"This is a positive example.\",\n",
        "    \"I feel sad and lonely.\"\n",
        "]\n",
        "\n",
        "# 텍스트 전처리\n",
        "tokenized_inputs = preprocess_texts(texts)\n",
        "\n",
        "# BiLSTM 모델 빌드\n",
        "bilstm_model = build_bilstm_model(albert_model)\n",
        "\n",
        "# 모델 출력 확인\n",
        "features = bilstm_model(tokenized_inputs)\n",
        "print(features.shape)  # (batch_size, lstm_units)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVsUNE2wcAN",
        "outputId": "7ec22400-8656-4d73-d3e0-784e58eb0d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']\n",
            "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 128)\n"
          ]
        }
      ]
    }
  ]
}