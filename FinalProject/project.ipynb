{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 매 시도마다 돌릴 것들"
      ],
      "metadata": {
        "id": "g7qZZJCL_NvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install datasets --quiet\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/data/'  # 이미지 및 텍스트 파일 폴더\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoConfig\n",
        "from scipy.special import softmax\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMkpM8rP_RdE",
        "outputId": "b68234e5-da58-4554-c999-661cbb01a95f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "qz8G_fuW_v1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 데이터 로드\n",
        "texts_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/texts.csv', encoding='unicode_escape')\n",
        "label_txt = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/label.txt', sep='\\t', header=0)\n",
        "label_txt.columns = ['ID', 'Annotator1', 'Annotator2', 'Annotator3']\n",
        "label_jpg = label_txt.copy()\n",
        "\n",
        "def pre_text(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "texts_df['Text'] = texts_df['Text'].apply(pre_text)"
      ],
      "metadata": {
        "id": "A_k8XYMJifKc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {'positive': 1, 'neutral':0, 'negative': -1}\n",
        "def pre_labeltext(text):\n",
        "    new_text = text.split(\",\")[0]\n",
        "    return dic[new_text]\n",
        "label_txt['Annotator1'] = label_txt['Annotator1'].apply(pre_labeltext)\n",
        "label_txt['Annotator2'] = label_txt['Annotator2'].apply(pre_labeltext)\n",
        "label_txt['Annotator3'] = label_txt['Annotator3'].apply(pre_labeltext)\n",
        "label_txt['label'] = label_txt['Annotator1'] + label_txt['Annotator2'] + label_txt['Annotator3']\n",
        "label_txt['label'] = label_txt['label']/3\n",
        "label_txt = label_txt.drop(['Annotator1', 'Annotator2', 'Annotator3'], axis=1)"
      ],
      "metadata": {
        "id": "JZqXgtrVAiYc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_labeltext(text):\n",
        "    new_text = text.split(\",\")[1]\n",
        "    return dic[new_text]\n",
        "label_jpg['Annotator1'] = label_jpg['Annotator1'].apply(pre_labeltext)\n",
        "label_jpg['Annotator2'] = label_jpg['Annotator2'].apply(pre_labeltext)\n",
        "label_jpg['Annotator3'] = label_jpg['Annotator3'].apply(pre_labeltext)\n",
        "label_jpg['label'] = label_jpg['Annotator1'] + label_jpg['Annotator2'] + label_jpg['Annotator3']\n",
        "label_jpg['label'] = label_jpg['label']/3\n",
        "label_jpg = label_jpg.drop(['Annotator1', 'Annotator2', 'Annotator3'], axis=1)"
      ],
      "metadata": {
        "id": "IqwSsrVkApKb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨 데이터와 텍스트 데이터 병합\n",
        "merged_text = pd.merge(label_txt, texts_df, on='ID')"
      ],
      "metadata": {
        "id": "DretnLVMj9K-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 라벨링-모델 일치율 확인(만족스럽다)"
      ],
      "metadata": {
        "id": "rdeeeqfcT90t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def checksentiment(listtexts, listlabels):\n",
        "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "    model.to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "    config = AutoConfig.from_pretrained(MODEL)\n",
        "    batch_size = 16\n",
        "    sentiment_scores = []\n",
        "\n",
        "    for i in tqdm(range(0, len(listtexts), batch_size)):\n",
        "        batch_texts = listtexts[i:i+batch_size]\n",
        "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        encoded_input.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_input)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        scores = probabilities.cpu().numpy()\n",
        "\n",
        "        pos_probs = scores[:, 2]  # 긍정 확률\n",
        "        neg_probs = scores[:, 0]  # 부정 확률\n",
        "        sentiments = pos_probs - neg_probs  # 감성 점수\n",
        "        sentiment_scores.extend(sentiments)\n",
        "\n",
        "    listlabels = np.array(listlabels)\n",
        "    sentiment_scores = np.array(sentiment_scores)\n",
        "\n",
        "    # L2 Loss\n",
        "    l2_loss = np.mean((sentiment_scores - listlabels) ** 2)\n",
        "    print(f\"L2 Loss: {l2_loss}\")\n",
        "\n",
        "    return sentiment_scores"
      ],
      "metadata": {
        "id": "hIYkkKkX7KE1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab = merged_text['label'].tolist()\n",
        "sentiment_scores = checksentiment(merged_text['Text'].tolist(), lab)\n",
        "check = 0\n",
        "for i in range(len(sentiment_scores)):\n",
        "    if sentiment_scores[i] * lab[i] < 0:\n",
        "        check+=1\n",
        "check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxdBHP7A8BAB",
        "outputId": "539be6fc-d4b5-41dd-da22-4a9af7a0d1e8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/1225 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "100%|██████████| 1225/1225 [00:15<00:00, 77.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Loss: 0.2285643614866811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1713"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 설명-감성 분석 점수 확인(학습 전 저성능 확인)"
      ],
      "metadata": {
        "id": "rMZJzuuVUFOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#자기 전에 켜 둘 것\n",
        "source_dir = '/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/data'\n",
        "\n",
        "target_dir = '/content/data'\n",
        "\n",
        "# 대상 폴더가 없으면 생성\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "loader = label_txt['ID'].tolist()\n",
        "\n",
        "for filename in tqdm(loader):\n",
        "    filename_with_ext = f\"{filename}.jpg\"\n",
        "    src_file = os.path.join(source_dir, filename_with_ext)\n",
        "    dst_file = os.path.join(target_dir, filename_with_ext)\n",
        "\n",
        "    try:\n",
        "        shutil.copy2(src_file, dst_file)\n",
        "    except Exception as e:\n",
        "        print(f\"파일 복사 중 오류 발생: {filename_with_ext}, 오류: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrWxnT8MfbA7",
        "outputId": "7c74d253-ebd3-43f1-bbd7-2ed240f83767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 6599/19598 [22:53<46:44,  4.64it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged = merged_text"
      ],
      "metadata": {
        "id": "RmJ6xRF48I9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = pd.merge(merged, label_txt, on='ID')"
      ],
      "metadata": {
        "id": "JIsC6wiJ83DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = pd.merge(merged, label_jpg, on='ID')"
      ],
      "metadata": {
        "id": "aILzOBlxts8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = np.array(sentiment_scores)\n",
        "sim = merged['label_x'] * merged['label_y'] > 0\n",
        "merged['pred'] = scores\n",
        "input_df = merged[sim]"
      ],
      "metadata": {
        "id": "Z5VMAsSNXP2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_df.drop([1336], axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "uylOq6zrGzHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_df.drop([623], axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "f4VcwFc67VMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_df.drop([12183], axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "sxJVrxhqGDgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/data'\n",
        "def jpg(num):\n",
        "    return f\"/content/data/{num}.jpg\"\n",
        "input_df['ID'] = input_df['ID'].apply(jpg)"
      ],
      "metadata": {
        "id": "GcDaSHbVhSR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_df = input_df.astype({'ID':'str'})"
      ],
      "metadata": {
        "id": "3fKKolPn7cO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_df = input_df.drop('label_y', axis=1)\n",
        "input_df = input_df.drop('label_x', axis=1)\n",
        "input_df = input_df.drop('Unnamed: 2', axis=1)\n",
        "input_df = input_df.drop('label', axis=1)"
      ],
      "metadata": {
        "id": "LVo6hZABrzEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_df = input_df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "MGTINN1NtCJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataframe = input_df[0:10000].reset_index(drop=True)\n",
        "val_dataframe = input_df[10000:13000].reset_index(drop=True)\n",
        "test_dataframe = input_df[13000:15816].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Esd1bPCQuTPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine tuning"
      ],
      "metadata": {
        "id": "JkpKoUMqS81k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "train_dataset = []\n",
        "for _, row in tqdm(train_dataframe.iterrows()):\n",
        "    image_path = row[\"ID\"]\n",
        "    text = row[\"Text\"]\n",
        "\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # 이미지 로드\n",
        "        train_dataset.append({\"image\": image, \"text\": text})  # 이미지와 텍스트 저장\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "\n",
        "# 데이터 확인\n",
        "print(\"Dataset length:\", len(train_dataset))\n",
        "print(\"Sample data:\", train_dataset[0])"
      ],
      "metadata": {
        "id": "Cal6CtJPEhyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = []\n",
        "for _, row in tqdm(val_dataframe.iterrows()):\n",
        "    image_path = row[\"ID\"]\n",
        "    text = row[\"Text\"]\n",
        "\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # 이미지 로드\n",
        "        val_dataset.append({\"image\": image, \"text\": text})  # 이미지와 텍스트 저장\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "\n",
        "# 데이터 확인\n",
        "print(\"Dataset length:\", len(val_dataset))\n",
        "print(\"Sample data:\", val_dataset[0])"
      ],
      "metadata": {
        "id": "38OCO8dAFx2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ImageCaptioningDataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
        "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
        "        return encoding"
      ],
      "metadata": {
        "id": "xO65ltefTA8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
      ],
      "metadata": {
        "id": "TDLCNpw5TM14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = ImageCaptioningDataset(train_dataset, processor)\n",
        "train_dataloader = DataLoader(train, shuffle=False, batch_size=8)"
      ],
      "metadata": {
        "id": "qUVRb_AKUJhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = ImageCaptioningDataset(val_dataset, processor)\n",
        "val_dataloader = DataLoader(val, shuffle=False, batch_size=8)"
      ],
      "metadata": {
        "id": "-q_0LaDgu979"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "iUAr5d6FZyO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "-cOJR6t9Ujqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 5\n",
        "min_loss = 1000000"
      ],
      "metadata": {
        "id": "9VV1XLfEvLpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    for idx, batch in tqdm(enumerate(train_dataloader)):\n",
        "        input_ids = batch.pop(\"input_ids\").to(device)\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        pixel_values=pixel_values,\n",
        "                        labels=input_ids)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    print('train loss:', total_loss)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    for idx, batch in enumerate(tqdm(val_dataloader)):\n",
        "        input_ids = batch.pop(\"input_ids\").to(device)\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        pixel_values=pixel_values,\n",
        "                        labels=input_ids)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print('val loss:', total_loss)\n",
        "\n",
        "    if min_loss > total_loss:\n",
        "        min_loss = total_loss\n",
        "        count = 5\n",
        "    else:\n",
        "        count -= 1\n",
        "    if count == 0:\n",
        "        break"
      ],
      "metadata": {
        "id": "ntIJKytiU7cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 결과 확인(기본 loss 0.5~0.4가 기본)"
      ],
      "metadata": {
        "id": "JaHVSrMa-BVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 감성 분석 모델 로드 (고정된 상태로 사용)\n",
        "MODEL_SA = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer_sa = AutoTokenizer.from_pretrained(MODEL_SA)\n",
        "config_sa = AutoConfig.from_pretrained(MODEL_SA)\n",
        "model_sa = AutoModelForSequenceClassification.from_pretrained(MODEL_SA)\n",
        "model_sa.to(device)\n",
        "model_sa.eval()  # 평가 모드\n",
        "\n",
        "# 감성 분석 모델의 파라미터를 고정\n",
        "for param in model_sa.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P2omBdYgKGW",
        "outputId": "b1051b7c-6698-4f26-8905-66f494c5df14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 정의 (예: MSELoss)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# 옵티마이저 정의 (BLIP 모델의 파라미터만 업데이트)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "iQkrW_9lg10-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100  # 최대 에포크 수를 크게 설정\n",
        "patience = 5      # 검증 손실이 개선되지 않는 에포크 수 허용치\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "early_stop = False"
      ],
      "metadata": {
        "id": "ibxsISvyhQtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_ids, labels, image_dir, processor):\n",
        "        self.image_ids = image_ids\n",
        "        self.labels = labels\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, self.image_ids[idx])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        # 이미지 전처리 수행\n",
        "        pix = self.processor(image, return_tensors='pt')\n",
        "        # 배치 차원 제거 (processor는 보통 배치 차원을 포함하여 반환)\n",
        "        pix = {k: v.squeeze(0) for k, v in pix.items()}\n",
        "        label = self.labels[idx]\n",
        "        return pix, label\n",
        "\n",
        "# 데이터셋과 데이터로더 생성\n",
        "test_id = test_dataframe['ID'].tolist()\n",
        "test_label = test_dataframe['pred'].tolist()\n",
        "test_dataset = ImageDataset(test_id, test_label, image_dir, processor)\n",
        "\n",
        "batch_size = 8  # 적절한 배치 사이즈 설정\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Pw_UDQRukOoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for batch in tqdm(test_dataloader):\n",
        "    pix, labels = batch\n",
        "    pix = {k: v.to(device) for k, v in pix.items()}\n",
        "    labels = labels.to(device).float()\n",
        "\n",
        "    # BLIP 모델을 통해 캡션 생성\n",
        "    outputs = model.generate(**pix)\n",
        "    # 생성된 토큰 ID를 디코딩하여 텍스트로 변환\n",
        "    captions = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "    # 감성 분석 모델을 통해 감성 점수 계산\n",
        "    inputs_sa = tokenizer_sa(captions, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs_sa = model_sa(**inputs_sa)\n",
        "    scores = outputs_sa.logits.cpu().numpy()\n",
        "    probs = softmax(scores, axis=1)\n",
        "    pos_probs = probs[:, 2]  # 긍정 확률 (모델에 따라 인덱스 확인 필요)\n",
        "    neg_probs = probs[:, 0]  # 부정 확률\n",
        "    sentiment_scores = pos_probs - neg_probs  # [batch_size]\n",
        "\n",
        "    # 감성 점수를 텐서로 변환\n",
        "    sentiment_scores = torch.tensor(sentiment_scores, dtype=torch.float).to(device)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "avg_loss = total_loss / len(test_dataloader)\n",
        "print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "CjTrGSs4BJ5K",
        "outputId": "798e37d3-4fc4-4c93-a266-d059f25ba0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/362 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "100%|██████████| 362/362 [26:01<00:00,  4.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 24.9462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41/41 [02:52<00:00,  4.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 25.5248\n",
            "Validation loss improved. Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 362/362 [26:02<00:00,  4.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Loss: 24.9436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41/41 [02:52<00:00,  4.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 25.5248\n",
            "No improvement in validation loss for 1 epoch(s).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 5/362 [00:24<29:29,  4.96s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-4e19f5cb364e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# BLIP 모델을 통해 캡션 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# 생성된 토큰 ID를 디코딩하여 텍스트로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, interpolate_pos_encoding, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         outputs = self.text_decoder.generate(\n\u001b[0m\u001b[1;32m   1196\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_initial_cache_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3195\u001b[0;31m         while self._has_unfinished_sequences(\n\u001b[0m\u001b[1;32m   3196\u001b[0m             \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mthis_peer_finished_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2413\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2414\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}