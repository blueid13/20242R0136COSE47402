{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 매 시도마다 돌릴 것들"
      ],
      "metadata": {
        "id": "g7qZZJCL_NvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install datasets --quiet\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/data/'  # 이미지 및 텍스트 파일 폴더\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoConfig\n",
        "from scipy.special import softmax\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMkpM8rP_RdE",
        "outputId": "d8319213-1883-41da-9dbb-45e8530488d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "qz8G_fuW_v1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 데이터 로드\n",
        "texts_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/texts.csv', encoding='unicode_escape')\n",
        "label_txt = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/label.txt', sep='\\t', header=0)\n",
        "label_txt.columns = ['ID', 'Annotator1', 'Annotator2', 'Annotator3']\n",
        "label_jpg = label_txt.copy()\n",
        "\n",
        "def pre_text(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "texts_df['Text'] = texts_df['Text'].apply(pre_text)"
      ],
      "metadata": {
        "id": "A_k8XYMJifKc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {'positive': 1, 'neutral':0, 'negative': -1}\n",
        "def pre_labeltext(text):\n",
        "    new_text = text.split(\",\")[0]\n",
        "    return dic[new_text]\n",
        "label_txt['Annotator1'] = label_txt['Annotator1'].apply(pre_labeltext)\n",
        "label_txt['Annotator2'] = label_txt['Annotator2'].apply(pre_labeltext)\n",
        "label_txt['Annotator3'] = label_txt['Annotator3'].apply(pre_labeltext)\n",
        "label_txt['label'] = label_txt['Annotator1'] + label_txt['Annotator2'] + label_txt['Annotator3']\n",
        "label_txt['label'] = label_txt['label']/3\n",
        "label_txt = label_txt.drop(['Annotator1', 'Annotator2', 'Annotator3'], axis=1)"
      ],
      "metadata": {
        "id": "JZqXgtrVAiYc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_labeltext(text):\n",
        "    new_text = text.split(\",\")[1]\n",
        "    return dic[new_text]\n",
        "label_jpg['Annotator1'] = label_jpg['Annotator1'].apply(pre_labeltext)\n",
        "label_jpg['Annotator2'] = label_jpg['Annotator2'].apply(pre_labeltext)\n",
        "label_jpg['Annotator3'] = label_jpg['Annotator3'].apply(pre_labeltext)\n",
        "label_jpg['label'] = label_jpg['Annotator1'] + label_jpg['Annotator2'] + label_jpg['Annotator3']\n",
        "label_jpg['label'] = label_jpg['label']/3\n",
        "label_jpg = label_jpg.drop(['Annotator1', 'Annotator2', 'Annotator3'], axis=1)"
      ],
      "metadata": {
        "id": "IqwSsrVkApKb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨 데이터와 텍스트 데이터 병합\n",
        "merged_text = pd.merge(label_txt, texts_df, on='ID')"
      ],
      "metadata": {
        "id": "DretnLVMj9K-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 라벨링-모델 일치율 확인(만족스럽다)"
      ],
      "metadata": {
        "id": "rdeeeqfcT90t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def checksentiment(listtexts, listlabels):\n",
        "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "    model.to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "    config = AutoConfig.from_pretrained(MODEL)\n",
        "    batch_size = 16\n",
        "    sentiment_scores = []\n",
        "\n",
        "    for i in tqdm(range(0, len(listtexts), batch_size)):\n",
        "        batch_texts = listtexts[i:i+batch_size]\n",
        "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        encoded_input.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_input)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        scores = probabilities.cpu().numpy()\n",
        "\n",
        "        pos_probs = scores[:, 2]  # 긍정 확률\n",
        "        neg_probs = scores[:, 0]  # 부정 확률\n",
        "        sentiments = pos_probs - neg_probs  # 감성 점수\n",
        "        sentiment_scores.extend(sentiments)\n",
        "\n",
        "    listlabels = np.array(listlabels)\n",
        "    sentiment_scores = np.array(sentiment_scores)\n",
        "\n",
        "    # L2 Loss\n",
        "    l2_loss = np.mean((sentiment_scores - listlabels) ** 2)\n",
        "    print(f\"L2 Loss: {l2_loss}\")\n",
        "\n",
        "    return sentiment_scores"
      ],
      "metadata": {
        "id": "hIYkkKkX7KE1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab = merged_text['label'].tolist()\n",
        "sentiment_scores = checksentiment(merged_text['Text'].tolist(), lab)\n",
        "check = 0\n",
        "for i in range(len(sentiment_scores)):\n",
        "    if sentiment_scores[i] * lab[i] < 0:\n",
        "        check+=1\n",
        "check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxdBHP7A8BAB",
        "outputId": "ce2bb2c5-94c5-4403-c35a-ba464ab3a72f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/1225 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "100%|██████████| 1225/1225 [00:20<00:00, 58.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Loss: 0.22856436414728568\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1713"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 설명-감성 분석 점수 확인(학습 전 저성능 확인)"
      ],
      "metadata": {
        "id": "rMZJzuuVUFOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#자기 전에 켜 둘 것\n",
        "source_dir = '/content/drive/MyDrive/Colab Notebooks/COSE474/MVSA/data'\n",
        "\n",
        "target_dir = '/content/data'\n",
        "\n",
        "# 대상 폴더가 없으면 생성\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "loader = label_txt['ID'].tolist()\n",
        "\n",
        "for filename in tqdm(loader):\n",
        "    filename_with_ext = f\"{filename}.jpg\"\n",
        "    src_file = os.path.join(source_dir, filename_with_ext)\n",
        "    dst_file = os.path.join(target_dir, filename_with_ext)\n",
        "\n",
        "    try:\n",
        "        shutil.copy2(src_file, dst_file)\n",
        "    except Exception as e:\n",
        "        print(f\"파일 복사 중 오류 발생: {filename_with_ext}, 오류: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrWxnT8MfbA7",
        "outputId": "d672cec1-864b-4761-a4e9-d4ae53591f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19598/19598 [1:50:31<00:00,  2.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged = pd.merge(merged_text, label_jpg, on='ID')"
      ],
      "metadata": {
        "id": "RmJ6xRF48I9t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = pd.merge(merged, label_txt, on='ID')"
      ],
      "metadata": {
        "id": "JIsC6wiJ83DR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = np.array(sentiment_scores)\n",
        "sim = merged['label_x'] * merged['label_y'] > 0\n",
        "merged['pred'] = scores\n",
        "input_df = merged[sim]"
      ],
      "metadata": {
        "id": "Z5VMAsSNXP2h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_df.drop([1336], axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "uylOq6zrGzHw",
        "outputId": "508f71c1-aeb9-4c24-ac13-cc8517aaef25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-a5e798a59b6a>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  input_df.drop([1336], axis=0, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/data'\n",
        "def jpg(num):\n",
        "    return f\"{num}.jpg\"\n",
        "image_ids = input_df['ID'].apply(jpg).tolist()\n",
        "actual_labels = input_df['pred'].tolist()\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model_blip = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model_blip.to(device)"
      ],
      "metadata": {
        "id": "GcDaSHbVhSR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f5443b-713b-44be-b122-99070a2f4413"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BlipForConditionalGeneration(\n",
              "  (vision_model): BlipVisionModel(\n",
              "    (embeddings): BlipVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (encoder): BlipEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x BlipEncoderLayer(\n",
              "          (self_attn): BlipAttention(\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): BlipMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (text_decoder): BlipTextLMHeadModel(\n",
              "    (bert): BlipTextModel(\n",
              "      (embeddings): BlipTextEmbeddings(\n",
              "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (encoder): BlipTextEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BlipTextLayer(\n",
              "            (attention): BlipTextAttention(\n",
              "              (self): BlipTextSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): BlipTextSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BlipTextAttention(\n",
              "              (self): BlipTextSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): BlipTextSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BlipTextIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BlipTextOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (cls): BlipTextOnlyMLMHead(\n",
              "      (predictions): BlipTextLMPredictionHead(\n",
              "        (transform): BlipTextPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (transform_act_fn): GELUActivation()\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "captions = []\n",
        "\n",
        "for image_id in tqdm(image_ids):\n",
        "\n",
        "    image_path = os.path.join(image_dir, image_id)\n",
        "\n",
        "    # 이미지 로드 및 전처리\n",
        "    try:\n",
        "        raw_image = Image.open(image_path).convert('RGB')\n",
        "    except Exception as e:\n",
        "        print(f\"이미지 로드 실패: {image_id}, 오류: {e}\")\n",
        "        captions.append(\"\")  # 빈 문자열 추가하여 인덱스 맞추기\n",
        "        continue\n",
        "\n",
        "    # 입력 데이터 생성\n",
        "    inputs = processor(images=raw_image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # 캡션 생성\n",
        "    with torch.no_grad():\n",
        "        out = model_blip.generate(**inputs)\n",
        "\n",
        "    # 캡션 디코딩\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "    captions.append(caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "L_aYgG8WhdxF",
        "outputId": "f4891341-65f5-4ea5-b97f-c20225a82001"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/12842 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  1%|          | 153/12842 [00:50<1:09:39,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-38a78c024d39>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# 캡션 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_blip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# 캡션 디코딩\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, interpolate_pos_encoding, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         outputs = self.text_decoder.generate(\n\u001b[0m\u001b[1;32m   1196\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, return_logits, is_decoder, reduction)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, is_decoder)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    784\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    435\u001b[0m                 )\n\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    438\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             cross_attention_outputs = self.crossattention(\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         )\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/blip/modeling_blip_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         )\n\u001b[0;32m-> 2900\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2901\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_scores2 = checksentiment(captions, actual_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtz7tvnRAqIu",
        "outputId": "b2b66437-ce45-4c92-ad4a-dd8460f13d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/804 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "100%|██████████| 804/804 [00:08<00:00, 90.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Loss: 0.5017479144824811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check = 0\n",
        "for i in range(len(actual_labels)):\n",
        "    if (abs(sentiment_scores2[i] - actual_labels[i]) > 0.4) or (sentiment_scores2[i] * actual_labels[i] < 0):\n",
        "        check+=1\n",
        "check"
      ],
      "metadata": {
        "id": "kvZPoOFTbIAl",
        "outputId": "04f57b91-16ef-4f26-9400-acd507261b1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9873"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(sentiment_scores, bins=50, kde=True)"
      ],
      "metadata": {
        "id": "HRi1kKhMc-h1",
        "outputId": "accd2191-dd42-4af8-ec98-9b407b6850ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='Count'>"
            ]
          },
          "metadata": {},
          "execution_count": 168
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 멀티모달 이미지 감성-설명 학습"
      ],
      "metadata": {
        "id": "JaHVSrMa-BVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = actual_labels\n",
        "image_dir = '/content/data'"
      ],
      "metadata": {
        "id": "7IEa7zwhdy8I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids\n",
        "actual_labels"
      ],
      "metadata": {
        "id": "HbR5CLvZswJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "blip_model.to(device)"
      ],
      "metadata": {
        "id": "TjjuipiIgKCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor(Image.open(os.path.join(image_dir, image_ids[0])).convert('RGB'))"
      ],
      "metadata": {
        "id": "tUtd2xDKs_h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_ids, val_image_ids, train_labels, val_labels = train_test_split(image_ids, actual_labels, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "2irBzv3LtWlx"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 감성 분석 모델 로드 (고정된 상태로 사용)\n",
        "MODEL_SA = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer_sa = AutoTokenizer.from_pretrained(MODEL_SA)\n",
        "config_sa = AutoConfig.from_pretrained(MODEL_SA)\n",
        "model_sa = AutoModelForSequenceClassification.from_pretrained(MODEL_SA)\n",
        "model_sa.to(device)\n",
        "model_sa.eval()  # 평가 모드\n",
        "\n",
        "# 감성 분석 모델의 파라미터를 고정\n",
        "for param in model_sa.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P2omBdYgKGW",
        "outputId": "6e14c8ac-0dc1-4a39-a358-590078e0b766"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 정의 (예: MSELoss)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# 옵티마이저 정의 (BLIP 모델의 파라미터만 업데이트)\n",
        "optimizer = torch.optim.AdamW(blip_model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "iQkrW_9lg10-"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100  # 최대 에포크 수를 크게 설정\n",
        "patience = 5      # 검증 손실이 개선되지 않는 에포크 수 허용치\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "early_stop = False"
      ],
      "metadata": {
        "id": "ibxsISvyhQtx"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blip_model.eval()\n",
        "val_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(len(val_image_ids))):\n",
        "        pix = Image.open(os.path.join(image_dir, val_image_ids[i])).convert('RGB')\n",
        "        pix = processor(pix, return_tensors='pt')\n",
        "        pix = {k: v.to(device) for k, v in pix.items()}\n",
        "        labels = val_labels[i]  # [batch_size]\n",
        "        outputs = blip_model.generate(**pix)\n",
        "        captions = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "        inputs_sa = tokenizer_sa(captions, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        outputs_sa = model_sa(**inputs_sa)\n",
        "        scores = outputs_sa.logits\n",
        "        scores = scores.cpu().numpy()\n",
        "        probs = softmax(scores, axis=1)\n",
        "        pos_probs = probs[:, 2]\n",
        "        neg_probs = probs[:, 0]\n",
        "        sentiment_scores = pos_probs - neg_probs\n",
        "        sentiment_scores = torch.tensor(sentiment_scores, dtype=torch.float).to(device)\n",
        "\n",
        "        loss = abs(sentiment_scores - labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "avg_val_loss = val_loss / len(val_dataloader)\n",
        "print(f\"Validation Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "QTv-r-ZgB7yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    blip_model.train()\n",
        "    total_loss = 0.0\n",
        "    for i in tqdm(range(len(train_image_ids))):\n",
        "        pix = Image.open(os.path.join(image_dir, train_image_ids[i])).convert('RGB')\n",
        "        pix = processor(pix, return_tensors='pt')\n",
        "        pix = {k: v.to(device) for k, v in pix.items()}\n",
        "        labels = train_labels[i]  # [batch_size]\n",
        "        # BLIP 모델을 통해 캡션 생성\n",
        "        outputs = blip_model.generate(**pix)\n",
        "        # 생성된 토큰 ID를 디코딩하여 텍스트로 변환\n",
        "        captions = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "        blip_model.train()\n",
        "        # 감성 분석 모델을 통해 감성 점수 계산\n",
        "        inputs_sa = tokenizer_sa(captions, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs_sa = model_sa(**inputs_sa)\n",
        "        scores = outputs_sa.logits  # [batch_size, num_labels]\n",
        "\n",
        "        # 감성 점수 계산 (pos_probs - neg_probs)\n",
        "        scores = scores.cpu().numpy()\n",
        "        probs = softmax(scores, axis=1)\n",
        "        pos_probs = probs[:, 2]  # 긍정 확률\n",
        "        neg_probs = probs[:, 0]  # 부정 확률\n",
        "        sentiment_scores = pos_probs - neg_probs  # [batch_size]\n",
        "\n",
        "        # 감성 점수를 텐서로 변환\n",
        "        sentiment_scores = torch.tensor(sentiment_scores, dtype=torch.float).to(device)\n",
        "\n",
        "        # 손실 계산 (예측된 감성 점수와 실제 라벨 간의 차이)\n",
        "        loss = abs(sentiment_scores - labels)\n",
        "        loss.requires_grad_(True)\n",
        "        # 역전파 및 옵티마이저 스텝\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # 검증 루프\n",
        "    blip_model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(val_image_ids))):\n",
        "            pix = Image.open(os.path.join(image_dir, val_image_ids[i])).convert('RGB')\n",
        "            pix = processor(pix, return_tensors='pt')\n",
        "            pix = {k: v.to(device) for k, v in pix.items()}\n",
        "            labels = val_labels[i]  # [batch_size]\n",
        "            outputs = blip_model.generate(**pix)\n",
        "            captions = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "            inputs_sa = tokenizer_sa(captions, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "            outputs_sa = model_sa(**inputs_sa)\n",
        "            scores = outputs_sa.logits\n",
        "            scores = scores.cpu().numpy()\n",
        "            probs = softmax(scores, axis=1)\n",
        "            pos_probs = probs[:, 2]\n",
        "            neg_probs = probs[:, 0]\n",
        "            sentiment_scores = pos_probs - neg_probs\n",
        "            sentiment_scores = torch.tensor(sentiment_scores, dtype=torch.float).to(device)\n",
        "\n",
        "            loss = abs(sentiment_scores - labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early Stopping 체크\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        # 최적의 모델 저장 (옵션)\n",
        "        torch.save(blip_model.state_dict(), 'best_blip_model.pth')\n",
        "        print(\"Validation loss improved. Model saved.\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"No improvement in validation loss for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "Ew2b09oDg19c",
        "outputId": "d01d30bd-10ca-47e9-82dc-0926372db247"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/11557 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "100%|██████████| 11557/11557 [1:02:50<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 2.3986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1285 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-b7c5d5461efa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0msentiment_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3780\u001b[0m         )\n\u001b[0;32m-> 3781\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3782\u001b[0m         warnings.warn(\n\u001b[1;32m   3783\u001b[0m             \u001b[0;34mf\"Using a target size ({target.size()}) that is different to the input size ({input.size()}). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_ids, labels, image_dir, processor):\n",
        "        self.image_ids = image_ids\n",
        "        self.labels = labels\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, self.image_ids[idx])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        # 이미지 전처리 수행\n",
        "        pix = self.processor(image, return_tensors='pt')\n",
        "        # 배치 차원 제거 (processor는 보통 배치 차원을 포함하여 반환)\n",
        "        pix = {k: v.squeeze(0) for k, v in pix.items()}\n",
        "        label = self.labels[idx]\n",
        "        return pix, label\n",
        "\n",
        "# 데이터셋과 데이터로더 생성\n",
        "train_dataset = ImageDataset(train_image_ids, train_labels, image_dir, processor)\n",
        "val_dataset = ImageDataset(val_image_ids, val_labels, image_dir, processor)\n",
        "\n",
        "batch_size = 32  # 적절한 배치 사이즈 설정\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Pw_UDQRukOoy"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    blip_model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        pix, labels = batch\n",
        "        pix = {k: v.to(device) for k, v in pix.items()}\n",
        "        labels = labels.to(device).float()\n",
        "\n",
        "        # BLIP 모델을 통해 캡션 생성\n",
        "        outputs = blip_model.generate(**pix)\n",
        "        # 생성된 토큰 ID를 디코딩하여 텍스트로 변환\n",
        "        captions = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "        # 감성 분석 모델을 통해 감성 점수 계산\n",
        "        inputs_sa = tokenizer_sa(captions, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs_sa = model_sa(**inputs_sa)\n",
        "        scores = outputs_sa.logits.cpu().numpy()\n",
        "        probs = softmax(scores, axis=1)\n",
        "        pos_probs = probs[:, 2]  # 긍정 확률 (모델에 따라 인덱스 확인 필요)\n",
        "        neg_probs = probs[:, 0]  # 부정 확률\n",
        "        sentiment_scores = pos_probs - neg_probs  # [batch_size]\n",
        "\n",
        "        # 감성 점수를 텐서로 변환\n",
        "        sentiment_scores = torch.tensor(sentiment_scores, dtype=torch.float).to(device)\n",
        "\n",
        "        # 손실 계산 (예: MSELoss 사용)\n",
        "        loss = criterion(sentiment_scores, labels)\n",
        "        loss.requires_grad_(True)\n",
        "        # 역전파 및 옵티마이저 스텝\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # 검증 루프\n",
        "    blip_model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader):\n",
        "            pix, labels = batch\n",
        "            pix = {k: v.to(device) for k, v in pix.items()}\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = blip_model.generate(**pix)\n",
        "            captions = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "            inputs_sa = tokenizer_sa(captions, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "            outputs_sa = model_sa(**inputs_sa)\n",
        "            scores = outputs_sa.logits.cpu().numpy()\n",
        "            probs = softmax(scores, axis=1)\n",
        "            pos_probs = probs[:, 2]\n",
        "            neg_probs = probs[:, 0]\n",
        "            sentiment_scores = pos_probs - neg_probs\n",
        "            sentiment_scores = torch.tensor(sentiment_scores, dtype=torch.float).to(device)\n",
        "\n",
        "            loss = criterion(sentiment_scores, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # 조기 종료 체크 및 모델 저장\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(blip_model.state_dict(), 'best_blip_model.pth')\n",
        "        print(\"Validation loss improved. Model saved.\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"No improvement in validation loss for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "CjTrGSs4BJ5K",
        "outputId": "23ede6e1-b231-46bd-ca1a-d043793a7f73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 72/362 [05:19<21:20,  4.42s/it]"
          ]
        }
      ]
    }
  ]
}